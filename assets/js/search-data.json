{
  
    
        "post0": {
            "title": "Title",
            "content": "# Revisiting Pandas &gt; This is a refreseher on the course &#39;Analyzing Police Activity with Pandas&#39; offered by Datacamp. - toc: true - branch: master - badges: true - comments: true - categories: [fastpages, jupyter] - image: images/some_folder/your_image.png - hide: false - search_exclude: true - metadata_key1: metadata_value1 - metadata_key2: metadata_value2 - author: Hamel Husain &amp; Jeremy Howard # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true - branch: master - badges: true - comments: true - - categories: [fastpages, jupyter] .",
            "url": "https://farru46.github.io/pyblog/2020/05/15/revisiting-pandas.html",
            "relUrl": "/2020/05/15/revisiting-pandas.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Supervised Learning with Scikit Learn",
            "content": "Table of Contents . 1&nbsp;&nbsp;Introduction | 2&nbsp;&nbsp;Classification2.1&nbsp;&nbsp;k-NN | 2.2&nbsp;&nbsp;Model Performance2.2.1&nbsp;&nbsp;Scoring | 2.2.2&nbsp;&nbsp;Overfitting and Underfitting | . | . | 3&nbsp;&nbsp;Regression3.1&nbsp;&nbsp;Using the Gapminder dataset for Linear Regression3.1.1&nbsp;&nbsp;R-squared | 3.1.2&nbsp;&nbsp;Train/Test Split For Regression | 3.1.3&nbsp;&nbsp;Model Evaluation | . | 3.2&nbsp;&nbsp;Cross Validation3.2.1&nbsp;&nbsp;Methodology | 3.2.2&nbsp;&nbsp;k-fold CV Comparison | . | 3.3&nbsp;&nbsp;Regularised Regression3.3.1&nbsp;&nbsp;Ridge Regression (L2)3.3.1.1&nbsp;&nbsp;Implementation in scikit-learn | . | 3.3.2&nbsp;&nbsp;Lasso Regresssion (L1)3.3.2.1&nbsp;&nbsp;Implementation in scikit-learn | . | . | . | 4&nbsp;&nbsp;Fine tuning your model4.1&nbsp;&nbsp;Confusion Matrix | 4.2&nbsp;&nbsp;Classification Report | 4.3&nbsp;&nbsp;Precision | 4.4&nbsp;&nbsp;Recall | 4.5&nbsp;&nbsp;Logistic Regression4.5.1&nbsp;&nbsp;Plotting an ROC Curve | 4.5.2&nbsp;&nbsp;Area Under the ROC Curve | . | 4.6&nbsp;&nbsp;Hyperparameter Tuning4.6.1&nbsp;&nbsp;Hyperparameter Tuning with GridSearchCV | 4.6.2&nbsp;&nbsp;Hyperparameter Tuning with RandomizedSearchCV | . | 4.7&nbsp;&nbsp;Hold-out set for final evaluation4.7.0.1&nbsp;&nbsp;Using a hold-out set for regression | 4.7.0.2&nbsp;&nbsp;Using a hold-out set for regression | . | . | . | 5&nbsp;&nbsp;Preprocessing and Pipelines5.1&nbsp;&nbsp;Dealing with categorical features5.1.1&nbsp;&nbsp;Distribution by category using boxplot | 5.1.2&nbsp;&nbsp;Creating dummy variables | 5.1.3&nbsp;&nbsp;Regression with categorical features | . | 5.2&nbsp;&nbsp;Handling Missing Values5.2.1&nbsp;&nbsp;Imputing missing values in an ML Pipeline5.2.1.1&nbsp;&nbsp;Setup a Pipeline | . | . | 5.3&nbsp;&nbsp;Centering and Scaling5.3.1&nbsp;&nbsp;Standardization | 5.3.2&nbsp;&nbsp;Centering and scaling in Pipeline | . | 5.4&nbsp;&nbsp;Pipelines for Classification | 5.5&nbsp;&nbsp;Pipelines for Regression | . | . Introduction . This post serves as a refresher for the course Supervised Learning with Scikit Learn offered by DataCamp. It will also serve as a guide for implememnting Classification and Regression Algorithms in Scikit learn. . Classification . In classification problems we are given labelled data and we need to fit a model to this data to predict the labels for new data . from sklearn import datasets import pandas as pd import numpy as np import matplotlib.pyplot as plt import janitor pd.set_option(&#39;display.max_rows&#39;, 1000) . k-NN . k-NN predicts the label of a data point by: . looking at k nearest labelled data points | Taking a majority vote | . We will apply k-NN on the iris dataset ussing the scikit fit and predict methods . The iris dataset can be loaded by the load_iris function from sklearn.datasets module. There are many more toy datasets in that module which can be used for practice. All of them come in the standard Bunch format which we will explore below. . from sklearn.datasets import load_iris . iris = load_iris() . type(iris) . sklearn.utils.Bunch . What is the bunch iris made of? . iris.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) . Get the data from the key data and labels from key target . X,y = iris.data, iris.target . Next ,we apply the k-NN on the iris dataset . from sklearn.neighbors import KNeighborsClassifier . knn= KNeighborsClassifier() . knn.fit(X,y) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . Using the KNN algorithm to predict on the original data . y_pred = knn.predict(X) . y_pred . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . Model Performance . Accuracy is an strong indicator of model performance (although this is not always the case). . Accuracy is defined as : . Accuracy = Number of correct predictions / Total number of data points . It&#39;s important to know how our model is faring on unseen data, i.e. completely new data on which the model is going to make predictions. For this purpose, ,the dataset is divided into two parts: . The training set | The test set | . Calculating accuracy on the test set will give us a better picture of how well our model generalizes to previously unseen data. . To calculate accuracy,, we use the model.score() method. . To divide our data into the train and test sets, ,use the train_test_split function from sklearn.model_selection module. . We will apply all these concepts on the digits dataset in the sklearn.datasets module. . First import the data . from sklearn.datasets import load_digits . digits=load_digits() . digits.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) . X,y=digits[&#39;data&#39;],digits[&#39;target&#39;] . plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) . &lt;matplotlib.image.AxesImage at 0x118ed9a58&gt; . Performing the train test split . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) . The parameter straitify=y is to ensure that the labels are distributed in the train and test set as they are in the original data . Using k-NN for multiclass classification . from sklearn.neighbors import KNeighborsClassifier . Fitting the classifier withn_neighbours=6 . knn= KNeighborsClassifier(n_neighbors=7) . knn.fit(X_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=7, p=2, weights=&#39;uniform&#39;) . y_pred = knn.predict(X_test) . Scoring . By default, accuracy gets reported . knn.score(X_test,y_test) . 0.9833333333333333 . Overfitting and Underfitting . If the model performs well on the train data but performs poorly on the test data, we say that the model has overfit the data and is not able to generalize to previously unseen data. | We plot model complexity curves to find the &#39;sweet spot&#39; of the model parameter, n_neighbours. | What value of the model parameter n_neighbours is the best? | . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train,y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . Regression . (Jump to Top) . Regression Problems deal with predicting continuos values like prices of houses, hourly temperature,etc. . from sklearn import datasets import pandas as pd import numpy as np import matplotlib.pyplot as plt import janitor pd.set_option(&#39;display.max_rows&#39;, 1000) . from sklearn.datasets import load_boston . boston=load_boston() . boston.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;]) . X,y=boston[&#39;data&#39;], boston[&#39;target&#39;] . boston_df = pd.DataFrame(boston[&#39;data&#39;], columns=boston.feature_names) . boston_df[&#39;price&#39;] = boston.target . boston_df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . X = boston_df.drop(&#39;price&#39;, axis=&#39;columns&#39;).values . y=boston_df[&#39;price&#39;].values . from sklearn.linear_model import LinearRegression . linreg = LinearRegression() . linreg.fit(X,y) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . linreg.coef_ . array([-1.08011358e-01, 4.64204584e-02, 2.05586264e-02, 2.68673382e+00, -1.77666112e+01, 3.80986521e+00, 6.92224640e-04, -1.47556685e+00, 3.06049479e-01, -1.23345939e-02, -9.52747232e-01, 9.31168327e-03, -5.24758378e-01]) . Using the Gapminder dataset for Linear Regression . In this problem, we try to predict the life expectancy based on different factors . from sklearn import datasets import pandas as pd import numpy as np import matplotlib.pyplot as plt import janitor import seaborn as sns pd.set_option(&#39;display.max_rows&#39;, 1000) . gapminder = pd.read_csv(&quot;../datasets/gapminder.csv&quot;) . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | Middle East &amp; North Africa | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | Sub-Saharan Africa | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | America | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | Europe &amp; Central Asia | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | East Asia &amp; Pacific | . gapminder.shape . (139, 10) . gapminder.drop(&#39;Region&#39;, axis=&#39;columns&#39;, inplace=True) . We will first try to predict the life expectancy on the basis of just one predictor viz fertility . X,y = gapminder.fertility.values, gapminder.life.values . X.shape . (139,) . We will have to do some reshaping since this is the format needed by scikit-learn . # Print the dimensions of X and y before reshaping print(f&quot;Dimensions of y before reshaping: {y.shape}&quot;) print(f&quot;Dimensions of X before reshaping: {X.shape}&quot;) . Dimensions of y before reshaping: (139,) Dimensions of X before reshaping: (139,) . # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) . # Print the dimensions of X and y after reshaping print(f&quot;Dimensions of y after reshaping: {y.shape}&quot;) print(f&quot;Dimensions of X after reshaping: {X.shape}&quot;) . Dimensions of y after reshaping: (139, 1) Dimensions of X after reshaping: (139, 1) . Let&#39;s lok at the correlations between life and other variables using seaborn&#39;s heatmap function . sns.heatmap(gapminder.corr(), square=True, cmap=&#39;RdYlGn&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11d30dc88&gt; . We can see that life and fertility are negatively correlated. Let&#39;s plot both to understand this . sns.relplot(x = &#39;fertility&#39;,y = &#39;life&#39;, data = gapminder) . &lt;seaborn.axisgrid.FacetGrid at 0x118d96748&gt; . Import the Linear Regression class from sklearn.linear_model . from sklearn.linear_model import LinearRegression . linreg=LinearRegression() . # Create the prediction space prediction_space = np.linspace(min(X), max(X)).reshape(-1,1) . linreg.fit(X,y) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . y_pred = linreg.predict(prediction_space) . R-squared . linreg.score(X,y) . 0.6192442167740035 . Visualizing the line of best fit . # Plot regression line plt.scatter(gapminder[&#39;fertility&#39;], gapminder[&#39;life&#39;]) plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . Train/Test Split For Regression . from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error . We will take all features here . X,y=gapminder.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder.life . X.shape, y.shape . ((139, 8), (139,)) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) . linreg=LinearRegression() . linreg.fit(X_train,y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . Model Evaluation . y_pred = linreg.predict(X_test) . print(f&quot;R^2 : {linreg.score(X_test,y_test)}&quot;) . R^2 : 0.8441876368902324 . rmse = np.sqrt(mean_squared_error(y_test,y_pred)) . print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) . Root Mean Squared Error: 3.204564021318649 . Cross Validation . The $R^2$ calculated above is dependent on the way the data is split in the train test split. | This arbitrary splitt into train and test may not be representative of the model&#39;s ability to generalize to new data. . | To counter this, we use cross validation. In this method, we split the data into k parts or folds, ,hence the name k-fold cross validation . | . Methodology . Let&#39;s take k = 5 for better understanding. . The data is divided intto 5 parts. | In the first iteration, first part is held out for testing and the model is trained on the remaining four parts. | $R^2$ is calculated on the first part | The same process is repeated taking other parts as the test set and $R^2$ is calculated on each of the test sets. | Final score = mean of all the scores | . from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score . linreg = LinearRegression() . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | . X,y=gapminder.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder.life . Now instead of applying the Linear Regression Model on X_train and y_train, we use 5-fold cross validation. Note the syntax . cv_scores = cross_val_score(linreg, X,y,cv=5) . cv_scores . array([0.81720569, 0.82917058, 0.90214134, 0.80633989, 0.94495637]) . Calculating average 5-fold CV score . import numpy as np . np.mean(cv_scores) . 0.8599627722793229 . k-fold CV Comparison . cv_scores_3=cross_val_score(linreg, X, y, cv=3) cv_scores_10 = cross_val_score(linreg, X,y, cv = 10) . cv_scores_3 . array([0.83699524, 0.87875694, 0.89986165]) . cv_scores_10 . array([0.73564531, 0.7988102 , 0.82780762, 0.86331179, 0.74902465, 0.94417158, 0.83259108, 0.78157196, 0.95777479, 0.94541964]) . np.mean(cv_scores_3) . 0.8718712782622106 . np.mean(cv_scores_10) . 0.8436128620131204 . %timeit cross_val_score(linreg, X, y, cv = 3) . 10 ms ± 490 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . %timeit cross_val_score(linreg, X, y, cv = 10) . 33.5 ms ± 1.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . Regularised Regression . In this section, I will be covering regularised regression which is a way to avoid overfitting. In particular, two methods will be discussed viz Ridge and Lasso . Large coefficients can lead to overfitting | Regularization is basically penalizing large coefficients | . Ridge Regression (L2) . Loss function = OLS function + $ alpha sum beta_i ^ 2 $ . Alpha or Lambda : Parameter that controls model complexitty | Alpha = 0, can cause overfitting | Alpha very high, can cause underfitting | . Implementation in scikit-learn . Let&#39;s load the data . import pandas as pd gapminder = pd.read_csv(&quot;../datasets/gapminder.csv&quot;) . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | Middle East &amp; North Africa | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | Sub-Saharan Africa | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | America | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | Europe &amp; Central Asia | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | East Asia &amp; Pacific | . Dropping the Region column . gapminder.drop(&#39;Region&#39;, inplace=True, axis=&#39;columns&#39;) . X,y = gapminder.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder.life . Let&#39;s import the Ridge class` . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score . alpha_space = np.logspace(-4, 0, 50) . alpha_space . array([1.00000000e-04, 1.20679264e-04, 1.45634848e-04, 1.75751062e-04, 2.12095089e-04, 2.55954792e-04, 3.08884360e-04, 3.72759372e-04, 4.49843267e-04, 5.42867544e-04, 6.55128557e-04, 7.90604321e-04, 9.54095476e-04, 1.15139540e-03, 1.38949549e-03, 1.67683294e-03, 2.02358965e-03, 2.44205309e-03, 2.94705170e-03, 3.55648031e-03, 4.29193426e-03, 5.17947468e-03, 6.25055193e-03, 7.54312006e-03, 9.10298178e-03, 1.09854114e-02, 1.32571137e-02, 1.59985872e-02, 1.93069773e-02, 2.32995181e-02, 2.81176870e-02, 3.39322177e-02, 4.09491506e-02, 4.94171336e-02, 5.96362332e-02, 7.19685673e-02, 8.68511374e-02, 1.04811313e-01, 1.26485522e-01, 1.52641797e-01, 1.84206997e-01, 2.22299648e-01, 2.68269580e-01, 3.23745754e-01, 3.90693994e-01, 4.71486636e-01, 5.68986603e-01, 6.86648845e-01, 8.28642773e-01, 1.00000000e+00]) . ridge_scores=[] ridge_scores_std=[] . # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha # ridge.alpha = alpha ridge = Ridge(alpha=alpha, normalize=True) # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge,X,y,cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std deviation of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) . # function to display plot def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . display_plot(ridge_scores, ridge_scores_std) . plt.plot(alpha_space) . Lasso Regresssion (L1) . Loss function = OLS function + $ alpha sum | beta_i| $ . Can be used to select important features of a dataset | Shrinks the coefficients of less important features to exactly 0 | Particularly useful when the dataset contains thousands of features aand we need to communicate to the business/industry only the most important feature(s). | . Implementation in scikit-learn . Importing the Lasso class . from sklearn.linear_model import Lasso . lasso = Lasso(alpha = 0.4, normalize=True) . Here we need to specify normalize=True since all the features should be on the same scale . lasso.fit(X,y) . Lasso(alpha=0.4, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False) . # Compute and print the coefficients lasso_coef = lasso.coef_ print(lasso_coef) # Plot the coefficients plt.plot(range(len(X.columns)), lasso_coef) plt.xticks(range(len(X.columns)), X.columns.values, rotation=60) plt.margins(0.02) plt.show() . [-0. -0. -0. 0. 0. 0. -0. -0.07087587] . So, ,we can say from the above chart that child mortality is the most important feature for predicting life expectancy . Fine tuning your model . (Jump to Top) . Accuracy may not always be the best metric to evaluate a classifciation model. | In imbalanced class classification problems we cannot always rely on accuracy as an indicator of model strength. | For example, if we have 100 emails, 10 of which are spam and 90 are ham, , a classsifier which classifies all emails as ham will have 90% accuracy, but this model does a poor job at identifying spam emails | For this reason we use the confusion matrix | To understand this, we will use the PIMA Indian Diabetes Dataset to predict which patients are more likely to get Diabetes. | . First, load the dataset . diabetes = pd.read_csv(&quot;../datasets/diabetes.csv&quot;) . diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . The goal here is to predict whether or not a given female patient will contract diabetes based on features such as BMI, Age, number of pregnancies etc. . Defining X and y . X,y = diabetes.drop(&#39;Outcome&#39;, axis = &#39;columns&#39;), diabetes.Outcome . Train/Test Split . from sklearn.model_selection import train_test_split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) . Fitting a k-NN model with k = 6 . from sklearn.neighbors import KNeighborsClassifier . knn=KNeighborsClassifier(n_neighbors=6) . knn.fit(X_train,y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=6, p=2, weights=&#39;uniform&#39;) . y_pred = knn.predict(X_test) . Confusion Matrix . A confusion matrix gives a pivot table between predicted and actual values | From the confusion matrix we can calculate other mterices like Precision, Recall and F-1 score as well | . from sklearn.metrics import confusion_matrix . print(confusion_matrix(y_test, y_pred)) . [[176 30] [ 56 46]] . Classification Report . from sklearn.metrics import classification_report . print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.76 0.85 0.80 206 1 0.61 0.45 0.52 102 accuracy 0.72 308 macro avg 0.68 0.65 0.66 308 weighted avg 0.71 0.72 0.71 308 . Precision . Precision = $ frac{TP}{(TP + FP)}$ | Basically, precision gives the percentage of correct predictions. . | For example, in the above case the confusion matrix came out to be : . | . print(confusion_matrix(y_test,y_pred)) . [[176 30] [ 56 46]] . This means that the model predicted a total of (46+30) = 76 1&#39;s, out of which 46 were correct predictions. | Hence the percentage of correct predictions by our model or precision will be $$ frac{46}{46+30} = 0.6052$$ . | Precision basically tells us that our model is correct 61% of the time . | . X.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | . Recall . Recall tells us oout of all the positive classes,, how many did our model predict correctly. | Recall = $ frac{TP}{TP+FN}$ | What percentage of all positive classes is our model able to classify correctly. | For example in the above example, total number of positive classes can be found out as | . y_test.value_counts() . 0 206 1 102 Name: Outcome, dtype: int64 . print(confusion_matrix(y_test,y_pred)) . [[176 30] [ 56 46]] . So there are 102 positive cases | Out of these our model is able to identify 46 correctly. | So recall can be calculated as : $$ frac{46}{46+56} = 0.45 $$ | . Logistic Regression . # load the dataset diabetes = pd.read_csv(&quot;../datasets/diabetes.csv&quot;) . diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . X,y = diabetes.drop(&#39;Outcome&#39;, axis = &#39;columns&#39;), diabetes.Outcome . 46/102 . 0.45098039215686275 . from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler . scale = StandardScaler() . X = scale.fit_transform(X) . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) . logreg = LogisticRegression() . logreg.fit(X_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . Predictiing on test set . y_pred = logreg.predict(X_test) . from sklearn.metrics import confusion_matrix, classification_report . print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[168 38] [ 37 65]] precision recall f1-score support 0 0.82 0.82 0.82 206 1 0.63 0.64 0.63 102 accuracy 0.76 308 macro avg 0.73 0.73 0.73 308 weighted avg 0.76 0.76 0.76 308 . Plotting an ROC Curve . To plot an ROC Curve we need the probabilities of occurrence of the positive class . from sklearn.metrics import roc_curve . y_pred_prob = logreg.predict_proba(X_test) y_pred_prob[0:5,:] . array([[0.69256175, 0.30743825], [0.80354496, 0.19645504], [0.8585552 , 0.1414448 ], [0.82720562, 0.17279438], [0.48856767, 0.51143233]]) . We need only the probabilities of the positive class . y_pred_prob = y_pred_prob[:,1] . fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) . Plot the ROC Curve . # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . Area Under the ROC Curve . Given the ROC Curve, what metric of interest can we extract? . Larger the area under the ROC Curve, the better the model . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) . AUC: 0.8251475347420522 . We can also use cross validation to get the auc . # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, scoring =&quot;roc_auc&quot;, cv = 5) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC scores computed using 5-fold cross-validation: [0.81240741 0.80925926 0.82537037 0.87339623 0.84377358] . A binary classifier which guesses randomly will be correct 50% of the time, since the TPR and FPR will always be equal in such a case | That means the area under the ROC curve , called as AUC , will be equal to 0.5 | Having AUC greater than 0.5 means that our model is better than random guessing, which is what we want | . Hyperparameter Tuning . Parameters which are specified before fitting the model are called hyperparameters | They cannot be learned by the model | . Hyperparameter Tuning with GridSearchCV . Let&#39;s load the diabetes dataset again . import pandas as pd . diabetes = pd.read_csv(&quot;../datasets/diabetes.csv&quot;) . diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . X,y = diabetes.drop(&#39;Outcome&#39;, axis=&#39;columns&#39;), diabetes.Outcome . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV . c_space = np.logspace(-5,8,15) . c_space . array([1.00000000e-05, 8.48342898e-05, 7.19685673e-04, 6.10540230e-03, 5.17947468e-02, 4.39397056e-01, 3.72759372e+00, 3.16227766e+01, 2.68269580e+02, 2.27584593e+03, 1.93069773e+04, 1.63789371e+05, 1.38949549e+06, 1.17876863e+07, 1.00000000e+08]) . param_grid = {&#39;C&#39;: c_space} . logreg = LogisticRegression() . logreg_cv = GridSearchCV(logreg, param_grid, cv=5) . from sklearn.preprocessing import scale X = scale(X) . logreg_cv.fit(X,y) . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;C&#39;: array([1.00000000e-05, 8.48342898e-05, 7.19685673e-04, 6.10540230e-03, 5.17947468e-02, 4.39397056e-01, 3.72759372e+00, 3.16227766e+01, 2.68269580e+02, 2.27584593e+03, 1.93069773e+04, 1.63789371e+05, 1.38949549e+06, 1.17876863e+07, 1.00000000e+08])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . What is best parameter value ? . logreg_cv.best_params_ . {&#39;C&#39;: 0.4393970560760795} . What is the best score ? . logreg_cv.best_score_ . 0.7721840251252015 . Hyperparameter Tuning with RandomizedSearchCV . from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV . param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} . tree = DecisionTreeClassifier() . tree_cv = RandomizedSearchCV(tree,param_dist, cv=5) . tree_cv.fit(X,y) . RandomizedSearchCV(cv=5, error_score=nan, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=None, splitter=&#39;best&#39;), i..._jobs=None, param_distributions={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_depth&#39;: [3, None], &#39;max_features&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x11dc553c8&gt;, &#39;min_samples_leaf&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x11db808d0&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=None, refit=True, return_train_score=False, scoring=None, verbose=0) . Best Hyperparameters . tree_cv.best_params_ . {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 6, &#39;min_samples_leaf&#39;: 3} . Best score . tree_cv.best_score_ . 0.7409387997623291 . Hold-out set for final evaluation . After tuning the hyperparameters, we need to score our model on data which the data has never seen before. | For this reason, we keep a hold-out set separately at the very beginning, so that it can later be used for evaluating our model with tuned hyperparameters. | . Using a hold-out set for regression . from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV . Creating the hyperparameter space. . Note that the parameter C denotes the inverse regularization parameter in Logistic Regression . c_space = np.logspace(-5,8,15) param_grid = {&quot;C&quot; : c_space, &#39;penalty&#39; : [&#39;l1&#39;,&#39;l2&#39;]} . Instantiate a logistic regression classifier . logreg=LogisticRegression() . Perform the train test split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) . logreg_cv = GridSearchCV(logreg, param_grid=param_grid, cv=5) . logreg_cv.fit(X_train,y_train) . /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. FitFailedWarning) . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;C&#39;: array([1.00000000e-05, 8.48342898e-05, 7.19685673e-04, 6.10540230e-03, 5.17947468e-02, 4.39397056e-01, 3.72759372e+00, 3.16227766e+01, 2.68269580e+02, 2.27584593e+03, 1.93069773e+04, 1.63789371e+05, 1.38949549e+06, 1.17876863e+07, 1.00000000e+08]), &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Best Hyperparameters . logreg_cv.best_params_ . {&#39;C&#39;: 0.05179474679231213, &#39;penalty&#39;: &#39;l2&#39;} . Best score . logreg_cv.best_score_ . 0.7652173913043477 . Using a hold-out set for regression . Write up about elastic net, l1-ratio etc . from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split . gapminder = pd.read_csv(&quot;../datasets/gapminder.csv&quot;).drop(&#39;Region&#39;, axis=&#39;columns&#39;) gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | . X,y = gapminder.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder.life . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) . Hyperparameter space . l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} . Instantiate the ElasticNet Regressor : elastic_net . elastic_net = ElasticNet() . gm_cv = GridSearchCV(elastic_net, param_grid=param_grid, cv=5) . gm_cv.fit(X_train,y_train) . /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 282.4862175850645, tolerance: 0.558941590909091 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 309.8466391486292, tolerance: 0.5893071666666668 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 255.5034400806133, tolerance: 0.5890250303030303 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 287.6728412633784, tolerance: 0.5814186865671642 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 311.1827114768189, tolerance: 0.5801944179104478 positive) . GridSearchCV(cv=5, error_score=nan, estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;l1_ratio&#39;: array([0. , 0.03448276, 0.06896552, 0.10344828, 0.13793103, 0.17241379, 0.20689655, 0.24137931, 0.27586207, 0.31034483, 0.34482759, 0.37931034, 0.4137931 , 0.44827586, 0.48275862, 0.51724138, 0.55172414, 0.5862069 , 0.62068966, 0.65517241, 0.68965517, 0.72413793, 0.75862069, 0.79310345, 0.82758621, 0.86206897, 0.89655172, 0.93103448, 0.96551724, 1. ])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . y_pred = gm_cv.predict(X_test) . r2 = gm_cv.score(X_test,y_test) . mse =mean_squared_error(y_test,y_pred) . print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.057914133398441 . Preprocessing and Pipelines . (Jump to Top) . Dealing with categorical features . Scikit does not accept non-numerical features. | To deal with categorical features, we have to convert them to numerical values. | This is achieved by converting the categorical variables to dummy variables. | Scikit&#39;s OneHotEncoder or pandas&#39; get_dummies is used for this purpose. | . import pandas as pd . gapminder = pd.read_csv(&quot;../datasets/gapminder.csv&quot;) . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | Middle East &amp; North Africa | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | Sub-Saharan Africa | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | America | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | Europe &amp; Central Asia | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | East Asia &amp; Pacific | . Here Region is a categorical variable . Distribution of life region wise . Distribution by category using boxplot . gapminder.boxplot(column=&quot;life&quot;,by = &#39;Region&#39;, rot= 45) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11db85dd8&gt; . Creating dummy variables . We will use pandas&#39; get_dummies function for this purpose . gapminder_region = pd.get_dummies(gapminder) . gapminder_region.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region_America Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia Region_Middle East &amp; North Africa Region_South Asia Region_Sub-Saharan Africa . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | 0 | 0 | 0 | 0 | 0 | 1 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | 1 | 0 | 0 | 0 | 0 | 0 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | 0 | 1 | 0 | 0 | 0 | 0 | . Just to illustrate . gapminder_region[&#39;Region&#39;] = gapminder.Region gapminder_region.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region_America Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia Region_Middle East &amp; North Africa Region_South Asia Region_Sub-Saharan Africa Region . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | 0 | 0 | 0 | 1 | 0 | 0 | Middle East &amp; North Africa | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | 0 | 0 | 0 | 0 | 0 | 1 | Sub-Saharan Africa | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | 1 | 0 | 0 | 0 | 0 | 0 | America | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | 0 | 0 | 1 | 0 | 0 | 0 | Europe &amp; Central Asia | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | 0 | 1 | 0 | 0 | 0 | 0 | East Asia &amp; Pacific | . Always a good option to drop oone category since it&#39;s obvious that it will be the category when nothing else holds . gapminder_region = pd.get_dummies(gapminder, drop_first=True) . gapminder_region.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia Region_Middle East &amp; North Africa Region_South Asia Region_Sub-Saharan Africa . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | 0 | 0 | 1 | 0 | 0 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | 0 | 0 | 0 | 0 | 1 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | 0 | 0 | 0 | 0 | 0 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | 0 | 1 | 0 | 0 | 0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | 1 | 0 | 0 | 0 | 0 | . Region_America is dropped now . Regression with categorical features . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score . ridge = Ridge(alpha=0.5, normalize=True) . Define X and y . X,y = gapminder_region.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder_region.life . ridge_cv = cross_val_score(ridge,X,y,cv=5) . print(ridge_cv) . [0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . Handling Missing Values . Load the diabetes dataset . diabetes = pd.read_csv(&quot;../datasets/diabetes.csv&quot;) . diabetes.isnull().sum() . Pregnancies 0 Glucose 0 BloodPressure 0 SkinThickness 0 Insulin 0 BMI 0 DiabetesPedigreeFunction 0 Age 0 Outcome 0 dtype: int64 . diabetes.head(10) . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . 5 5 | 116 | 74 | 0 | 0 | 25.6 | 0.201 | 30 | 0 | . 6 3 | 78 | 50 | 32 | 88 | 31.0 | 0.248 | 26 | 1 | . 7 10 | 115 | 0 | 0 | 0 | 35.3 | 0.134 | 29 | 0 | . 8 2 | 197 | 70 | 45 | 543 | 30.5 | 0.158 | 53 | 1 | . 9 8 | 125 | 96 | 0 | 0 | 0.0 | 0.232 | 54 | 1 | . BloddPressure, Insulin and SkinThickness cannot be 0. These indicate missing values. . diabetes.BloodPressure.replace(0,np.nan,inplace=True) diabetes.SkinThickness.replace(0,np.nan,inplace=True) diabetes.Insulin.replace(0,np.nan,inplace=True) diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72.0 | 35.0 | NaN | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66.0 | 29.0 | NaN | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64.0 | NaN | NaN | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66.0 | 23.0 | 94.0 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40.0 | 35.0 | 168.0 | 43.1 | 2.288 | 33 | 1 | . Print the number of Nan&#39;s - columnwise . diabetes.isnull().sum() . Pregnancies 0 Glucose 0 BloodPressure 35 SkinThickness 227 Insulin 374 BMI 0 DiabetesPedigreeFunction 0 Age 0 Outcome 0 dtype: int64 . What is the shape of original diabetes dataframe?` . diabetes.shape . (768, 9) . Drop all the rows with missing values . df=diabetes.dropna() . What is the shape of df? . df.shape . (394, 9) . Almost half of all the rows are dropped. This is NOT acceptable . Working with another dataframe voting . import pandas as pd import janitor import numpy as np . voting = pd.read_csv(&quot;../datasets/voting.csv&quot;).drop(&#39;Unnamed: 0&#39;, axis=&#39;columns&#39;).clean_names().drop(435, axis= &#39;rows&#39;) . voting.head() . handicapped_infants water_project_cost_sharing adoption_of_the_budget_resolution physician_fee_freeze el_salvador_aid religious_groups_in_schools anti_satellite_test_ban aid_to_nicaraguan_contras mx_missile immigration synfuels_corporation_cutback education_spending superfund_right_to_sue crime duty_free_exports export_administration_act_south_africa party . 0 n | y | n | y | y | y | n | n | n | y | ? | y | y | y | n | y | republican | . 1 n | y | n | y | y | y | n | n | n | n | n | y | y | y | n | ? | republican | . 2 ? | y | y | ? | y | y | n | n | n | n | y | n | y | y | n | n | democrat | . 3 n | y | y | n | ? | y | n | n | n | n | y | n | y | n | n | y | democrat | . 4 y | y | y | n | y | y | n | n | n | n | y | ? | y | y | y | y | democrat | . &#39;?&#39; indicates missing values . voting.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 435 entries, 0 to 434 Data columns (total 17 columns): handicapped_infants 435 non-null object water_project_cost_sharing 435 non-null object adoption_of_the_budget_resolution 435 non-null object physician_fee_freeze 435 non-null object el_salvador_aid 435 non-null object religious_groups_in_schools 435 non-null object anti_satellite_test_ban 435 non-null object aid_to_nicaraguan_contras 435 non-null object mx_missile 435 non-null object immigration 435 non-null object synfuels_corporation_cutback 435 non-null object education_spending 435 non-null object superfund_right_to_sue 435 non-null object crime 435 non-null object duty_free_exports 435 non-null object export_administration_act_south_africa 435 non-null object party 435 non-null object dtypes: object(17) memory usage: 61.2+ KB . voting[voting == &quot;?&quot;] = np.nan . voting.isnull().sum() . handicapped_infants 12 water_project_cost_sharing 48 adoption_of_the_budget_resolution 11 physician_fee_freeze 11 el_salvador_aid 15 religious_groups_in_schools 11 anti_satellite_test_ban 14 aid_to_nicaraguan_contras 15 mx_missile 22 immigration 7 synfuels_corporation_cutback 21 education_spending 31 superfund_right_to_sue 25 crime 17 duty_free_exports 28 export_administration_act_south_africa 104 party 0 dtype: int64 . print(f&quot;Shape of original dataframe: {voting.shape}&quot;) . Shape of original dataframe: (435, 17) . voting.head() . handicapped_infants water_project_cost_sharing adoption_of_the_budget_resolution physician_fee_freeze el_salvador_aid religious_groups_in_schools anti_satellite_test_ban aid_to_nicaraguan_contras mx_missile immigration synfuels_corporation_cutback education_spending superfund_right_to_sue crime duty_free_exports export_administration_act_south_africa party . 0 n | y | n | y | y | y | n | n | n | y | NaN | y | y | y | n | y | republican | . 1 n | y | n | y | y | y | n | n | n | n | n | y | y | y | n | NaN | republican | . 2 NaN | y | y | NaN | y | y | n | n | n | n | y | n | y | y | n | n | democrat | . 3 n | y | y | n | NaN | y | n | n | n | n | y | n | y | n | n | y | democrat | . 4 y | y | y | n | y | y | n | n | n | n | y | NaN | y | y | y | y | democrat | . Replace all &#39;n&#39; with 0 and &#39;y&#39; with 1 . voting[voting==&#39;n&#39;]=0 voting[voting==&#39;y&#39;]=1 . voting.head() . handicapped_infants water_project_cost_sharing adoption_of_the_budget_resolution physician_fee_freeze el_salvador_aid religious_groups_in_schools anti_satellite_test_ban aid_to_nicaraguan_contras mx_missile immigration synfuels_corporation_cutback education_spending superfund_right_to_sue crime duty_free_exports export_administration_act_south_africa party . 0 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | NaN | 1 | 1 | 1 | 0 | 1 | republican | . 1 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | NaN | republican | . 2 NaN | 1 | 1 | NaN | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | democrat | . 3 0 | 1 | 1 | 0 | NaN | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | democrat | . 4 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | NaN | 1 | 1 | 1 | 1 | democrat | . After dropping the rows containing missing values, . voting.dropna(axis=&#39;rows&#39;).shape . (232, 17) . Definitely not worth it dropping all the rows with NaN&#39;s. Half the data will be lost . Imputing missing values in an ML Pipeline . from sklearn.impute import SimpleImputer from sklearn.svm import SVC . imp = SimpleImputer(missing_values=np.nan, strategy=&#39;most_frequent&#39;) . Instantiating a Support Vector Classsifier . clf = SVC() . Setup a Pipeline . steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . # Same thing given above : steps = [(&#39;imputer&#39;, SimpleImputer(missing_values=np.nan, strategy=&#39;most_frequent&#39;)), (&#39;SVM&#39;, SVC())] . from sklearn.pipeline import Pipeline . pipeline=Pipeline(steps) . from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report . Define X and y . X,y = voting.drop(&#39;party&#39;, axis=&#39;columns&#39;), voting.party . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) . pipeline.fit(X_train,y_train) . Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;most_frequent&#39;, verbose=0)), (&#39;SVM&#39;, SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))], verbose=False) . y_pred = pipeline.predict(X_test) . print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.97 0.97 0.97 97 republican 0.94 0.94 0.94 47 accuracy 0.96 144 macro avg 0.95 0.95 0.95 144 weighted avg 0.96 0.96 0.96 144 . Centering and Scaling . Many ML algorithms use some form of distance to calculate similarity between observations, like k-NN | If the scale of the features vary widely, this may cause features on a larger scale influencing the model more heavily. | To counter this we use feature scaling, so that features can be on a similar scale. | . Standardization . $$ bar{x}= frac{x- mu}{ sigma}$$ . We will be using the white wine dataset to illustrate thses concepst . import pandas as pd import numpy as np import janitor . wine = pd.read_csv(&quot;../datasets/winequality-white.csv&quot;, sep=&quot;;&quot;) . wine.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . wine.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4898 entries, 0 to 4897 Data columns (total 12 columns): fixed acidity 4898 non-null float64 volatile acidity 4898 non-null float64 citric acid 4898 non-null float64 residual sugar 4898 non-null float64 chlorides 4898 non-null float64 free sulfur dioxide 4898 non-null float64 total sulfur dioxide 4898 non-null float64 density 4898 non-null float64 pH 4898 non-null float64 sulphates 4898 non-null float64 alcohol 4898 non-null float64 quality 4898 non-null int64 dtypes: float64(11), int64(1) memory usage: 459.3 KB . wine.describe() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . count 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | . mean 6.854788 | 0.278241 | 0.334192 | 6.391415 | 0.045772 | 35.308085 | 138.360657 | 0.994027 | 3.188267 | 0.489847 | 10.514267 | 5.877909 | . std 0.843868 | 0.100795 | 0.121020 | 5.072058 | 0.021848 | 17.007137 | 42.498065 | 0.002991 | 0.151001 | 0.114126 | 1.230621 | 0.885639 | . min 3.800000 | 0.080000 | 0.000000 | 0.600000 | 0.009000 | 2.000000 | 9.000000 | 0.987110 | 2.720000 | 0.220000 | 8.000000 | 3.000000 | . 25% 6.300000 | 0.210000 | 0.270000 | 1.700000 | 0.036000 | 23.000000 | 108.000000 | 0.991723 | 3.090000 | 0.410000 | 9.500000 | 5.000000 | . 50% 6.800000 | 0.260000 | 0.320000 | 5.200000 | 0.043000 | 34.000000 | 134.000000 | 0.993740 | 3.180000 | 0.470000 | 10.400000 | 6.000000 | . 75% 7.300000 | 0.320000 | 0.390000 | 9.900000 | 0.050000 | 46.000000 | 167.000000 | 0.996100 | 3.280000 | 0.550000 | 11.400000 | 6.000000 | . max 14.200000 | 1.100000 | 1.660000 | 65.800000 | 0.346000 | 289.000000 | 440.000000 | 1.038980 | 3.820000 | 1.080000 | 14.200000 | 9.000000 | . wine.quality.unique() . array([6, 5, 7, 8, 4, 3, 9]) . target_bool = wine[&#39;quality&#39;] &lt; 5 . wine.loc[target_bool,&#39;quality&#39;]=1 . wine.loc[~target_bool,&#39;quality&#39;]=0 . wine.quality.unique() . array([0, 1]) . Scaling the features . X,y = wine.drop(&#39;quality&#39;, axis=&#39;columns&#39;), wine.quality . from sklearn.preprocessing import scale . X_scaled = scale(X) . print(f&quot;Mean and Std dev of unscaled variables : {np.mean(X.values)}, {np.std(X.values)}&quot;) . Mean and Std dev of unscaled variables : 18.432687072460002, 41.54494764094571 . print(f&quot;Mean and Std dev of scaled variables : {np.mean(X_scaled)}, {np.std(X_scaled)}&quot;) . Mean and Std dev of scaled variables : 2.739937614267761e-15, 0.9999999999999999 . Centering and scaling in Pipeline . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier . steps = [(&#39;scaler&#39; , StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] . pipeline=Pipeline(steps) . from sklearn.model_selection import train_test_split . X,y = wine.drop(&#39;quality&#39;, axis=&#39;columns&#39;), wine.quality . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . knn_scaled = pipeline.fit(X_train,y_train) . knn_unscaled = KNeighborsClassifier().fit(X_train,y_train) . print(f&quot;scaled_score : {pipeline.score(X_test,y_test)}&quot;) print(f&quot;unscaled_score : {knn_unscaled.score(X_test,y_test)}&quot;) . scaled_score : 0.964625850340136 unscaled_score : 0.9666666666666667 . X_train . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol . 736 6.6 | 0.25 | 0.30 | 14.40 | 0.052 | 40.0 | 183.0 | 0.99800 | 3.02 | 0.50 | 9.1 | . 1620 7.8 | 0.26 | 0.49 | 3.20 | 0.027 | 28.0 | 87.0 | 0.99190 | 3.03 | 0.32 | 11.3 | . 336 6.3 | 0.23 | 0.33 | 1.50 | 0.036 | 15.0 | 105.0 | 0.99100 | 3.32 | 0.42 | 11.2 | . 2302 6.0 | 0.26 | 0.18 | 7.00 | 0.055 | 50.0 | 194.0 | 0.99591 | 3.21 | 0.43 | 9.0 | . 2673 7.9 | 0.37 | 0.31 | 2.85 | 0.037 | 5.0 | 24.0 | 0.99110 | 3.19 | 0.36 | 11.9 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4426 6.2 | 0.21 | 0.52 | 6.50 | 0.047 | 28.0 | 123.0 | 0.99418 | 3.22 | 0.49 | 9.9 | . 466 7.0 | 0.14 | 0.32 | 9.00 | 0.039 | 54.0 | 141.0 | 0.99560 | 3.22 | 0.43 | 9.4 | . 3092 7.6 | 0.27 | 0.52 | 3.20 | 0.043 | 28.0 | 152.0 | 0.99129 | 3.02 | 0.53 | 11.4 | . 3772 6.3 | 0.24 | 0.29 | 13.70 | 0.035 | 53.0 | 134.0 | 0.99567 | 3.17 | 0.38 | 10.6 | . 860 8.1 | 0.27 | 0.35 | 1.70 | 0.030 | 38.0 | 103.0 | 0.99255 | 3.22 | 0.63 | 10.4 | . 3428 rows × 11 columns . Pipelines for Classification . import pandas as pd import numpy as np . wine = pd.read_csv(&quot;../datasets/winequality-white.csv&quot;, sep=&quot;;&quot;) . target_bool = wine.quality &lt; 5 . wine.loc[target_bool, &#39;quality&#39;] = 1 wine.loc[~target_bool, &#39;quality&#39;] = 0 . wine.quality.unique() . array([0, 1]) . wine.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 0 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 0 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 0 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 0 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 0 | . X,y = wine.drop(&#39;quality&#39;, axis=&#39;columns&#39;), wine.quality . Building the pipeline . from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split . steps=[(&#39;scaler&#39;, StandardScaler()), (&#39;svc&#39;,SVC())] . pipeline = Pipeline(steps) . Hyperparameter space . parameters = {&#39;svc__C&#39;:[1, 10, 100], &#39;svc__gamma&#39;:[0.1, 0.01]} . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21) . cv = GridSearchCV(pipeline, param_grid=parameters, cv=3 ) . cv.fit(X_train,y_train) . GridSearchCV(cv=3, error_score=nan, estimator=Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svc&#39;, SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))], verbose=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;svc__C&#39;: [1, 10, 100], &#39;svc__gamma&#39;: [0.1, 0.01]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . y_pred = cv.predict(X_test) . # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.9693877551020408 precision recall f1-score support 0 0.97 1.00 0.98 951 1 0.43 0.10 0.17 29 accuracy 0.97 980 macro avg 0.70 0.55 0.58 980 weighted avg 0.96 0.97 0.96 980 Tuned Model Parameters: {&#39;svc__C&#39;: 100, &#39;svc__gamma&#39;: 0.01} . Pipelines for Regression . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, SimpleImputer(missing_values=np.nan, strategy=&#39;mean&#39;)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] . # Create the pipeline: pipeline pipeline = Pipeline(steps) . # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} . # Create train and test sets gapminder.drop(&#39;Region&#39;,axis=&#39;columns&#39;,inplace=True) X,y = gapminder.drop(&#39;life&#39;, axis=&#39;columns&#39;), gapminder.life X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42) . # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, param_grid=parameters, cv=3) . # Fit to the training set gm_cv.fit(X_train, y_train) . /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 734.4148861851631, tolerance: 0.4518648363636363 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 762.6366738748764, tolerance: 0.5123628000000002 positive) /Users/actify/environments/py_ds/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 763.9901914108555, tolerance: 0.4854468571428572 positive) . GridSearchCV(cv=3, error_score=nan, estimator=Pipeline(memory=None, steps=[(&#39;imputation&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;mean&#39;, verbose=0)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;elasticnet&#39;, ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5, max_iter=100... 0.17241379, 0.20689655, 0.24137931, 0.27586207, 0.31034483, 0.34482759, 0.37931034, 0.4137931 , 0.44827586, 0.48275862, 0.51724138, 0.55172414, 0.5862069 , 0.62068966, 0.65517241, 0.68965517, 0.72413793, 0.75862069, 0.79310345, 0.82758621, 0.86206897, 0.89655172, 0.93103448, 0.96551724, 1. ])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) . print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) . Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} Tuned ElasticNet R squared: 0.8862016570888216 .",
            "url": "https://farru46.github.io/pyblog/2020/05/15/classification-datacamp.html",
            "relUrl": "/2020/05/15/classification-datacamp.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "This is a sample notebook . #collapse-hide import pandas as pd import seaborn as sns . . tips=sns.load_dataset(&#39;tips&#39;) . tips . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | . 244 rows × 7 columns .",
            "url": "https://farru46.github.io/pyblog/2020/05/15/My-First-Post.html",
            "relUrl": "/2020/05/15/My-First-Post.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://farru46.github.io/pyblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://farru46.github.io/pyblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://farru46.github.io/pyblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://farru46.github.io/pyblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}